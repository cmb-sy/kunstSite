---
title: "(特徴)ガウス過程回帰を理解したい-Gaussian Process Regression"
date: "2022-02-22"
category: "ai"
tags:
  - "教師あり学習"
---

## はじめに

現在、ガウス過程を使った研究をしつつ勉強しているのですが、その備忘録として簡単にガウス過程の特徴についてまとめてみました。

研究や勉強を通してわかった特徴があれば随時更新していきたいです。

## ガウス過程における特徴

### 無限次元のガウス分布

別記事で述べてますが線形回帰モデルは、基底関数における特徴ベクトルの数を増やすことで複雑な関数も表現できます。

$$
f(x)=w_1\varphi_2(x)+w_2\varphi_2(x)+\dots+w_m\varphi_m(x)=\sum_{i}^{m}w_{i}\varphi_{i}(x)
$$

上記の線形回帰のモデルの式からわかるように 1 次元の$x$に対して$N$個の特徴ベクトルで考えるとき、それに対応する基底関数の数も$N$個必要になります。

このとき、$x$の次元が 2 次元の場合は基底関数は$N^2$個必要になり、3 次元の場合は$N^3$個必要になります。

このように$x$の次元が大きくなると必要な基底関数は指数関数的に増大します。

つまり、線形回帰のモデルは$x$の次元が低いときにのみ有効なのです。

それに対して、ガウス過程ではカーネルという手法を用いることによって、予測したい確率分布に(出力)にはパラメータ$w$が消えてなくなります。

ガウス過程回帰の式は以下の式でした。

$$
p(y^{\ast}|\mathbf{x}^{\ast},X,Y)=\mathcal{N}(\mathbf{k}^T_{\ast}\mathbf{K}^{-1}\mathbf{Y},\mathbf{k}_{\ast,\ast}-\mathbf{k}^T_{\ast}\mathbf{K}^{-1}\mathbf{k}_{\ast})
$$

$\mathbf{k}_{\ast}$は$X$と$\mathbf{x}^{\ast}$のカーネル、$\mathbf{K}$は$X$と$X$のカーネル、$\mathbf{K}_{\ast,\ast}$は$\mathbf{x}^{\ast}$と$\mathbf{x}^{\ast}$のカーネルです．

つまり、ガウス過程においては$x$や$\phi(\mathbf{x})$の次元をどれだけ大きくしようとも、予測される分布(多変量ガウス分布)はその影響を受けません。

こういったことからガウス過程は無限次元のガウス分布と呼ばれるのです。

したがって、ガウス過程では特徴ベクトルを使った表記だと無限個の特徴ベクトルが必要になるものを、カーネル関数一つで表わすことができるという特徴があります。

### 入力$x$が似ていれば出力$y$も似ている

ガウス過程では、カーネル関数を利用することで線形回帰モデルの高次元の問題を解決して予測分布における平均と分散を算出しています。

カーネル関数は$k(x,x^{\prime})=\boldsymbol \varphi(x)^T\boldsymbol \varphi(x^{\prime})$で表せられて、特徴ベクトルの内積になっています。

つまり、2 つの入力$x,x^{\prime}$間での２つの出力$y, y^{\prime}$の類似度を計算することで、ガウス過程回帰の共分散行列を求めています。

こういったことからガウス過程には、入力$x$が似ていれば出力$y$も似ているという特徴があります。

### 計算オーダーが大きい

グラム行列の計算が$O(N^2)$のオーダーであったり、逆行列は要素数$n$に対して$O(N)^3$の計算量を持っています。こういったことからガウス過程の手法では計算量が多いというデメリットがあります。

そのため計算オーダーを小さくする方法は色々研究されています。

### 様々なガウス過程

ガウス過程の一種として次のものが一般的に知られています。

- ウィーナー過程
- オルンシュタイン＝ウーレンベック過程
- 深層学習

## 終わりに

ここまで読んでくださってありがとうございました。

編集リクエストもお待ちしています。
